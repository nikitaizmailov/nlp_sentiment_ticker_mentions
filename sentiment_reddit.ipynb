{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971fb4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# for decoding the downloaded files\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8db3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making API call to PushShift API\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0029474",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"seo\"\n",
    "url = f\"https://api.pushshift.io/reddit/search/comment/?q={query}\"\n",
    "request = requests.get(url)\n",
    "json_response = request.json()\n",
    "json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d03acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pushshift_data(data_type, **kwargs):\n",
    "    \"\"\"\n",
    "    Gets data from the pushshift api.\n",
    " \n",
    "    data_type can be 'comment' or 'submission'\n",
    "    The rest of the args are interpreted as payload.\n",
    " \n",
    "    Read more: https://github.com/pushshift/api\n",
    "    \"\"\"\n",
    " \n",
    "    base_url = f\"https://api.pushshift.io/reddit/search/{data_type}/\"\n",
    "    payload = kwargs\n",
    "    request = requests.get(base_url, params=payload)\n",
    "    return request.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13308c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type=\"comment\"     # give me comments, use \"submission\" to publish something\n",
    "query=\"python\"          # Add your query\n",
    "duration=\"30d\"          # Select the timeframe. Epoch value or Integer + \"s,m,h,d\" (i.e. \"second\", \"minute\", \"hour\", \"day\")\n",
    "size=1000               # maximum 1000 comments\n",
    "sort_type=\"score\"       # Sort by score (Accepted: \"score\", \"num_comments\", \"created_utc\")\n",
    "sort=\"desc\"             # sort descending\n",
    "aggs=\"subreddit\"        #\"author\", \"link_id\", \"created_utc\", \"subreddit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a25c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_pushshift_data(data_type=data_type,\n",
    "                          q=query,\n",
    "                          after=duration,\n",
    "                          size=size,\n",
    "                          aggs=aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec431adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f994d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grouped = data_df.groupby(\"subreddit_name_prefixed\").agg({\"subreddit_id\":\"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grouped = data_grouped.sort_values(\"subreddit_id\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grouped = data_grouped.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib\n",
    "plt.figure(figsize=(16,9))\n",
    "\n",
    "plt.bar(data_grouped['subreddit_name_prefixed'][0:10], data_grouped['subreddit_id'][0:10])\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Frequency of comments\")\n",
    "plt.xlabel(\"Subreddits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f6ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    " \n",
    "px.bar(data_grouped,              # our dataframe\n",
    "       x=\"subreddit_name_prefixed\",         # x will be the 'key' column of the dataframe\n",
    "       y=\"subreddit_id\",   # y will be the 'doc_count' column of the dataframe\n",
    "       title=f'Subreddits with most activity - comments with \"{query}\" in the last \"{duration}\"',\n",
    "       labels={\"doc_count\": \"# comments\",\"key\": \"Subreddits\"}, # the axis names\n",
    "       color_discrete_sequence=[\"#1f77b4\"], # the colors used\n",
    "       height=500,\n",
    "       width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699380a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"comment\"\n",
    "query_word = \"VIAC\"\n",
    "aggs = \"subreddit\"\n",
    "after = 1\n",
    "subreddit = \"wallstreetbets\"\n",
    "sort = \"desc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to query last 1000 comments from any subreddit\n",
    "\n",
    "def extract_comments(data_type, **kwargs):\n",
    "    base_url = f\"https://api.pushshift.io/reddit/search/{data_type}\"\n",
    "    payload = kwargs\n",
    "    request = requests.get(base_url, params=payload)\n",
    "    return request.url, request.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a7a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "comms = extract_comments(data_type = data_type,\n",
    "                  subreddit=subreddit,\n",
    "                  aggs=aggs,\n",
    "                  sort=sort,\n",
    "                  after=\"1d\",\n",
    "                  size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "comms2 = extract_comments(data_type = data_type,\n",
    "                  subreddit=subreddit,\n",
    "                  aggs=aggs,\n",
    "                  sort=sort,\n",
    "                  after=\"1d\",\n",
    "                  size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08d92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comms2_df = pd.DataFrame(comms2[1]['data'])\n",
    "comms2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea917cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c22634",
   "metadata": {},
   "outputs": [],
   "source": [
    "comms[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9279f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df = pd.DataFrame(comms[1]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df = comms_df[['author', 'body', 'subreddit', 'subreddit_name_prefixed', 'permalink', 'created_utc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b1ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7253ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utc_to_date_obj(date):\n",
    "    str_date = time.strftime(\"%Y-%m-%d\", time.localtime(date))\n",
    "    date_obj = datetime.strptime(str_date, \"%Y-%m-%d\").date()\n",
    "    return date_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ed2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df['created_utc2'] = comms_df['created_utc'].apply(lambda x: utc_to_date_obj(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4261cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df.drop(labels=['created_utc'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df.rename(columns={\"created_utc2\":\"timestamp\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7903ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "db = 'wsb_data'\n",
    "db_tbl_name = 'wsb_comments'\n",
    "\n",
    "'''\n",
    "Create a mapping of df dtypes to mysql data types (not perfect, but close enough)\n",
    "'''\n",
    "def dtype_mapping():\n",
    "    return {'object' : 'TEXT',\n",
    "        'int64' : 'INT',\n",
    "        'float64' : 'FLOAT',\n",
    "        'datetime64' : 'DATETIME',\n",
    "        'bool' : 'TINYINT',\n",
    "        'category' : 'TEXT',\n",
    "        'timedelta[ns]' : 'TEXT'}\n",
    "\n",
    "'''\n",
    "Create a sqlalchemy engine\n",
    "'''\n",
    "def mysql_engine(user = 'root', password = 'Blackstar5140!', host = '127.0.0.1', port = '3306', database = db):\n",
    "    engine = create_engine(\"mysql://{0}:{1}@{2}:{3}/{4}?charset=utf8mb4\".format(user, password, host, port, database))\n",
    "    return engine\n",
    "\n",
    "'''\n",
    "Create a mysql connection from sqlalchemy engine\n",
    "'''\n",
    "def mysql_conn(engine):\n",
    "    conn = engine.raw_connection()\n",
    "    return conn\n",
    "\n",
    "'''\n",
    "Create sql input for table names and types\n",
    "'''\n",
    "def gen_tbl_cols_sql(df):\n",
    "    dmap = dtype_mapping()\n",
    "    sql = \"comment_id INT AUTO_INCREMENT PRIMARY KEY\"\n",
    "    df1 = df.rename(columns = {\"\" : \"nocolname\"})\n",
    "    hdrs = df1.dtypes.index\n",
    "    hdrs_list = [(hdr, str(df1[hdr].dtype)) for hdr in hdrs]\n",
    "    for i, hl in enumerate(hdrs_list):\n",
    "        sql += \" ,{0} {1}\".format(hl[0], dmap[hl[1]])\n",
    "    return sql\n",
    "\n",
    "'''\n",
    "Create a mysql table from a df\n",
    "'''\n",
    "def create_mysql_tbl_schema(df, conn, db, tbl_name):\n",
    "    tbl_cols_sql = gen_tbl_cols_sql(df)\n",
    "    sql = \"USE {0}; CREATE TABLE {1} ({2})\".format(db, tbl_name, tbl_cols_sql)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    cur.close()\n",
    "    conn.commit()\n",
    "\n",
    "'''\n",
    "Write df data to newly create mysql table\n",
    "'''\n",
    "def df_to_mysql(df, engine, tbl_name):\n",
    "    df.to_sql(tbl_name, engine, if_exists='append', method='multi')\n",
    "\n",
    "# df = comms_df.copy()\n",
    "# # create_mysql_tbl_schema(df, mysql_conn(mysql_engine()), db, db_tbl_name)\n",
    "# df_to_mysql(df, mysql_engine(), db_tbl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853fa689",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_engine = mysql_engine()\n",
    "sql_connection = sql_engine.connect()\n",
    "comms_df.to_sql(db_tbl_name, sql_engine, if_exists='append', method='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487967bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af6c66f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the data from the MySQL database\n",
    "# SQL Code to run\n",
    "sql_code = text(\"\"\"\n",
    "select * \n",
    "from wsb_data.wsb_comments;\n",
    "\"\"\")\n",
    "\n",
    "# creating an engine with attributes provided\n",
    "sql_engine = mysql_engine()\n",
    "# establishing connection with the database\n",
    "sql_connection = sql_engine.connect()\n",
    "# Executing the sql code.\n",
    "extracted_data = sql_connection.execute(sql_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_database = pd.read_sql(sql_code, sql_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f407670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3942ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When making changes to the database in the form of making updates, deletions or inserts, you need to commit the changes.\n",
    "# creating an engine with attributes provided\n",
    "sql_engine2 = mysql_engine()\n",
    "# establishing connection with the database. Need to use raw_connection()\n",
    "sql_connection2 = sql_engine2.raw_connection()\n",
    "\n",
    "# cursor in order to write sql queries with changes that are being made to the database. Not just select and extract keywords.\n",
    "cursor2 = sql_connection2.cursor()\n",
    "cursor2.execute(\"USE wsb_data; DROP TABLE test_tb, test_tb2, test_tb3;\")\n",
    "cursor2.close()\n",
    "sql_connection2.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e3646f",
   "metadata": {},
   "source": [
    "## Use this part below to download the comments from reddit with specific subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea342622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmaw import PushshiftAPI\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8696b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_delta = dt.timedelta(days=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ca7adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = dt.datetime.now() - days_delta\n",
    "start_date = int(start_date.timestamp())\n",
    "start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = dt.datetime.now() - dt.timedelta(days=17)\n",
    "end_date = int(end_date.timestamp())\n",
    "end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting subreddit to pull comments from and number of comments to pull in form of a limit value\n",
    "subreddit=\"wallstreetbets\"\n",
    "limit=40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c173a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Please expect to take some time to download this, i.e. the bigger the number like 100,000 then expect like 10 minutes download time due to reddit api 100 comments per request limit.\n",
    "comments_data = api.search_comments(subreddit=subreddit, limit=limit, before=end_date, after=start_date)\n",
    "\n",
    "print(f'Retrieved {len(comments_data)} comments from Pushshift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ea440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here stores the downloaded comments into a dataframe.\n",
    "full_df = pd.DataFrame(comments_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e33d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4c9a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the downloaded data into a csv file to store the data.\n",
    "# you can change the name of the file to whatever you want.\n",
    "full_df.to_csv('./wsb_comments4.csv', header=True, index=False, columns=list(full_df.axes[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe52274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing a csv file to put in the database\n",
    "df_put = pd.read_csv(\"wsb_comments4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ae0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51468ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data to the database\n",
    "df_put = df_put[['author', 'body', 'subreddit', 'subreddit_name_prefixed', 'permalink', 'created_utc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_put['created_utc'] = df_put['created_utc'].apply(lambda x: utc_to_date_obj(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc2add",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_put.rename(columns={\"created_utc\":\"timestamp\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_put.columns\n",
    "list(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_mysql(df_put, mysql_engine(), db_tbl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from the MySQL database\n",
    "# SQL Code to run\n",
    "sql_code = text(\"\"\"\n",
    "select * \n",
    "from wsb_data.wsb_comments;\n",
    "\"\"\")\n",
    "\n",
    "# creating an engine with attributes provided\n",
    "sql_engine = mysql_engine()\n",
    "# establishing connection with the database\n",
    "sql_connection = sql_engine.connect()\n",
    "# Executing the sql code.\n",
    "extracted_data = sql_connection.execute(sql_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f4c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments_df = pd.DataFrame(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00749f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Data to put in correct from in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f997dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_column(column_delete, df):\n",
    "    df.drop(labels=[column_delete], axis=1, inplace=True)\n",
    "    return df\n",
    "all_comments_df = drop_column(int(0), all_comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a73a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns\n",
    "all_comments_df.columns = ['author',\n",
    " 'body',\n",
    " 'subreddit',\n",
    " 'subreddit_name_prefixed',\n",
    " 'permalink',\n",
    " 'timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca4a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d314285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check for duplicate comments and drop them if found.\n",
    "check_dupes = all_comments_df.loc[all_comments_df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f6f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dupes.loc[check_dupes['body'] == \"Its only insulting to white liberals.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4547a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the first occurence of duplicates and dropping the subsequent occurence of them rows\n",
    "def drop_dupes(df):\n",
    "    indicies_of_rows = df.loc[df.duplicated(keep='last')].index\n",
    "    indicies_of_rows = list(indicies_of_rows)\n",
    "    df.drop(labels=indicies_of_rows, axis=0, inplace=True)\n",
    "    return df\n",
    "all_comments_df = drop_dupes(all_comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba519016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows where the author and comment body has been deleted\n",
    "def drop_author_body_empty(df):\n",
    "    rows_empty = df.loc[(df['author']==\"[deleted]\") | df['body']==\"[deleted]\"].index\n",
    "    rows_empty = list(rows_empty)\n",
    "    df.drop(labels=rows_empty, axis=0, inplace=True)\n",
    "    return df\n",
    "all_comments_df = drop_author_body_empty(all_comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fda2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see now we don't have duplicated rows\n",
    "all_comments_df.loc[all_comments_df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for empty strings/NaNs or Nulls\n",
    "all_comments_df.loc[(all_comments_df['body'] == 'None') | (pd.isna(all_comments_df['body']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b77867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting empty rows\n",
    "def delete_empty(df):\n",
    "    rows_to_del = df.loc[(df['body'] == 'None') | (pd.isna(df['body']))].index\n",
    "    df.drop(labels=rows_to_del, axis=0, inplace=True)\n",
    "    return df\n",
    "all_comments_df = delete_empty(all_comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3819d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments_df = all_comments_df.reset_index(drop=True)\n",
    "all_comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bd422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal1: Extracting from each comment ticker mentions. DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfff5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a NLP Pipeline.\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5588760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = all_comments_df['body'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6292f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c20b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take a look what the model has found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc3332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "labels = []\n",
    "position_start = []\n",
    "position_end = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    entities.append(ent)\n",
    "    labels.append(ent.label_)\n",
    "    position_start.append(ent.start_char)\n",
    "    position_end.append(ent.end_char)\n",
    "\n",
    "df = pd.DataFrame({\"Entities\":entities,\"Labels\":labels, \"Position_start\":position_start, \"Position_end\":position_end})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae1002",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82839de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a function that will look at every comment and extract everything useful from each comment.\n",
    "def extract_entities(text_string, BLACKLIST=[]):\n",
    "    # we are passing BLACKLIST variable to exclude arbitrary values you want\n",
    "    doc = nlp(text_string)\n",
    "    org_list = []\n",
    "    for entity in doc.ents:\n",
    "        # Here we check if the picked up entity is the Organization and whether it is also not in our blacklist variable\n",
    "        if entity.label_ == 'ORG' and entity.text.lower() not in BLACKLIST:\n",
    "            org_list.append(str(entity.text).upper())\n",
    "    # if organization is identified more than once it will appear multiple times in list\n",
    "    # we use set() to remove duplicates then convert back to list\n",
    "    # we also do this in order to prevent users to spam the same stock ticker to increase number of mentions for that ticker, hence one comment = one ticker mention of that company\n",
    "    org_list = list(set(org_list))\n",
    "    return org_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f31023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments_df['mentions'] = all_comments_df['body'].apply(lambda x: extract_entities(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56291f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Frequency Table\n",
    "all_orgs = all_comments_df['mentions'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403e6aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattening out the 2d array into 1d array\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_orgs = list(chain.from_iterable(all_orgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f11f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbeb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_freq = Counter(all_orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5866b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_freq = dict(org_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9201b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table = pd.DataFrame.from_dict(org_freq, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28255ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table.rename(columns={0:\"Frequency\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table = freq_table.sort_values(by=\"Frequency\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ea2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all US stock tickers and company names\n",
    "import io\n",
    "from get_all_tickers import get_tickers as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160418fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_of_tickers = \"http://ftp.nasdaqtrader.com/dynamic/SymDir/nasdaqtraded.txt\"\n",
    "tickers = requests.get(url_of_tickers)\n",
    "\n",
    "data_of_tickers = tickers.text\n",
    "data_content = tickers.content\n",
    "\n",
    "data_df = pd.read_csv(io.StringIO(data_content.decode(\"utf-8\")), sep=\"|\")\n",
    "data_df.drop(labels=[11853], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7168a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[data_df['Symbol'] == 'VIAC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c18903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_company_name(txt):\n",
    "    array = txt.split()\n",
    "    company_name = array[0].upper()\n",
    "    return company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d4bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Company_name'] = data_df['Security Name'].apply(lambda x: keep_company_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a2b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[data_df['Company_name']=='VIACOMCBS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creaating a dictionary\n",
    "stock_tickers = data_df['Symbol'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e84f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names = data_df['Company_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd3c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of company names and their respective tickers\n",
    "dict_stock_translator = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03131873",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(company_names)):\n",
    "    dict_stock_translator[company_names[i]] = dict_stock_translator.get(company_names[i], [])\n",
    "    dict_stock_translator[company_names[i]].append(stock_tickers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1602d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickers to company name\n",
    "tickers_company_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the conversion to have only stock tickers present.\n",
    "for key, value in dict_stock_translator.items():\n",
    "    for val in value:\n",
    "        tickers_company_dict[val] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff93564",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_company_dict['VIAC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68587fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_table = freq_table.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_table = copy_table.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc1bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_change(txt):\n",
    "    try:\n",
    "        txt = tickers_company_dict[txt]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2303aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_table['new_index'] = copy_table['index'].apply(lambda x: name_change(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_table.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_table.loc[copy_table['new_index'] == \"NVIDIA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_table.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f4066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by function to get the final frequency table\n",
    "copy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table_final = copy_table.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454d6717",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table_final.drop(labels=['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table_final['new_index'] = freq_table_final['new_index'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e8ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table_final = freq_table_final.groupby(\"new_index\")['Frequency'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d29d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table_final = freq_table_final.sort_values(by='Frequency', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table_final = freq_table_final.reset_index(drop=True)\n",
    "freq_table_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e4c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last filtering. Only keeping the financial stocks ONLY!\n",
    "freq_table_final = freq_table_final.loc[freq_table_final['new_index'].isin(list(dict_stock_translator))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee8fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising via Matplotlib and Plotly.\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4161b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20\n",
    "x_vals = freq_table_final['new_index'].tolist()[0:20]\n",
    "y_vals = freq_table_final['Frequency'].tolist()[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13108535",
   "metadata": {},
   "source": [
    "## Stock Mentions from all the comments provided in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae71b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,12))\n",
    "plt.bar(x_vals, y_vals, label='Stock Mentions')\n",
    "# plt.axis([0, -1, 0, 500])\n",
    "plt.ylabel(\"Number of Mentions\", fontsize=16)\n",
    "plt.xlabel(\"Company Stock\", fontsize=16)\n",
    "plt.xticks(fontsize=16, rotation=45)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.legend(loc=\"upper center\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search for specific company ticker and display it\n",
    "def display_ticker_mentions(ticker=None):\n",
    "    if (type(ticker) == int):\n",
    "        return\n",
    "    elif (len(ticker) < 5):\n",
    "        try:\n",
    "            company_name = tickers_company_dict[ticker]\n",
    "        except KeyError:\n",
    "            return \"Make Sure it is a real ticker\"\n",
    "    else:\n",
    "        company_name = ticker.upper()\n",
    "    \n",
    "    company_name = ticker.upper()\n",
    "    company_data = freq_table_final.loc[freq_table_final['new_index'] == company_name]['Frequency'][0]\n",
    "    \n",
    "    # plotting data\n",
    "    x_pos = [0.5]\n",
    "    y_vals = company_data\n",
    "    \n",
    "    plt.figure(figsize=(25,12))\n",
    "    plt.bar(x_pos, y_vals, label='Stock Mentions', width=0.1)\n",
    "    plt.ylabel(\"Number of Mentions\", fontsize=16)\n",
    "    plt.xlabel(\"Company Stock\", fontsize=16)\n",
    "    \n",
    "    plt.axis([0,1,0, company_data + 50])\n",
    "    \n",
    "    plt.xticks(x_pos, [company_name], fontsize=16, rotation=45)\n",
    "    plt.yticks(fontsize=16)\n",
    "    \n",
    "    \n",
    "    plt.legend(loc=\"upper center\", fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "display_ticker_mentions(ticker=\"GAMESTOP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b567306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal2: Classification of sentiment of the comment/sentence.\n",
    "# Starting SPARK NLP Preproccessing the text data\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee7aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spark-NLP Data Pipeline\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8656934",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_untouched_df = all_comments_df.copy()\n",
    "clean_untouched_df = clean_untouched_df[['body']]\n",
    "clean_untouched_df = clean_untouched_df['body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d91382",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_array_2d = np.array(clean_untouched_df)\n",
    "comments_array_2d = comments_array_2d.reshape(-1, 1)\n",
    "comments_array_2d = comments_array_2d.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    " spark_comments_df = spark.createDataFrame(comments_array_2d).toDF('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6dc10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using my own spark nlp pipeline\n",
    "# Using \n",
    "\n",
    "# initial stage of the preprocessing pipeline. needed to do all the below transformations.\n",
    "document = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# Breaks down sentence into list of words i.e. \"Nike is better than Adidas\" -> ['Nike', 'is', 'better', 'than', 'Adidas']\n",
    "token = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normal\")\n",
    "\n",
    "# The actual sentiment pretrained model I am using to classifiy the comments\n",
    "vivekn =  ViveknSentimentModel.pretrained() \\\n",
    "    .setInputCols([\"document\", \"normal\"]) \\\n",
    "    .setOutputCol(\"result_sentiment\")\n",
    "\n",
    "# Once we have our NLP pipeline ready to go, we might want to use our annotation results somewhere else where it is easy to use. The Finisher outputs annotation(s) values into a string.\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"result_sentiment\"]) \\\n",
    "    .setOutputCols(\"final_sentiment\")\n",
    "\n",
    "pipeline = Pipeline().setStages([document, token, normalizer, vivekn, finisher])\n",
    "\n",
    "pipelineModel = pipeline.fit(spark_comments_df)\n",
    "result = pipelineModel.transform(spark_comments_df)\n",
    "\n",
    "# result.select(\"final_sentiment\").show(truncate=False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da906508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the spark nlp df to a pandas df\n",
    "df_sentiment_comments = result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454ab3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame with all the comments and their respective positive/neutral/negative sentiment for each comment\n",
    "df_sentiment_comments['final_sentiment'] = df_sentiment_comments['final_sentiment'].astype(str)\n",
    "df_sentiment_comments['final_sentiment'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31463461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Sentiment of all the comments provided:\n",
    "df_overall_sentiment = df_sentiment_comments['final_sentiment'].value_counts()\n",
    "df_overall_sentiment = df_overall_sentiment.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd71845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Sentiment of the comments\n",
    "# As we can see we have more negative than positive comments\n",
    "df_overall_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7cf740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the sentiment data\n",
    "plt.figure(figsize=(14,6))\n",
    "neg_comments = df_sentiment_comments[\"final_sentiment\"][df_sentiment_comments[\"final_sentiment\"] == \"['negative']\"]\n",
    "pos_comments = df_sentiment_comments[\"final_sentiment\"][df_sentiment_comments[\"final_sentiment\"] == \"['positive']\"]\n",
    "neutral_comments = df_sentiment_comments[\"final_sentiment\"][df_sentiment_comments[\"final_sentiment\"] == \"['na']\"]\n",
    "unidentified_comments = df_sentiment_comments[\"final_sentiment\"][df_sentiment_comments[\"final_sentiment\"] == \"[]\"]\n",
    "plt.hist([\n",
    "         neg_comments,\n",
    "         pos_comments,\n",
    "         neutral_comments,\n",
    "         unidentified_comments,\n",
    "    ],\n",
    "     width=0.3,\n",
    "     label=[\"negative\", \"positive\", \"neutral\", \"na\"])\n",
    "plt.legend()\n",
    "plt.title(\"Sentiment of comments\")\n",
    "\n",
    "plt.xticks([0,1,2,3],['negative', 'positive', 'neutral', 'na'])\n",
    "plt.xlabel(\"Comment sentiment\")\n",
    "plt.ylabel(\"# of comments\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
